{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for better plots\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temos 1567 linhas/exemplos e 590 colunas/variáveis.\n",
      "A classe majoritária possui 1463 exemplos e a minoritária 104.\n"
     ]
    }
   ],
   "source": [
    "secom = pd.read_table('secom.data', header=None, delim_whitespace=True)\n",
    "Y = pd.read_table('secom_labels.data', header=None, usecols=[0], squeeze=True, delim_whitespace=True)\n",
    "\n",
    "print('Temos {} linhas/exemplos e {} colunas/variáveis.'\\\n",
    "      .format(secom.shape[0], secom.shape[1]))\n",
    "print('A classe majoritária possui {} exemplos e a minoritária {}.'\\\n",
    "      .format(Y[Y == -1].size, Y[Y == 1].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Remova colunas que tem mais que 30% de dados faltantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaram 558 colunas.\n"
     ]
    }
   ],
   "source": [
    "# vamos deixar 1096 valores por coluna, isto é, 70% de 1567, \n",
    "secom_30 = secom.dropna(thresh=1096, axis=1)\n",
    "print('Restaram %d colunas.' % secom_30.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remova as linhas que ainda tem dados faltantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secom formato (com linhas com dados faltantes): (1567, 558).\n",
      "Formato dos respectivos Labels: (1567, 1).\n",
      "Dados concatenados: (Secom + Labels): (1567, 559).\n",
      "Linhas com dados faltantes removidas. Novo número de linhas: 978.\n",
      "Novo formato do Secom (desconcatenado das Labels): (978, 558).\n",
      "Novo formato das Labels: (978,).\n"
     ]
    }
   ],
   "source": [
    "# transformar os dados do secom_30 em numpy-array\n",
    "X = secom_30.as_matrix(columns=None)\n",
    "\n",
    "# reshape nos labels para concatenar com X\n",
    "Y = Y.reshape(1567,1)\n",
    "\n",
    "# verificando as dimensões\n",
    "print('Secom formato (com linhas com dados faltantes): %r.\\nFormato dos respectivos Labels: %r.' % (X.shape, Y.shape))\n",
    "\n",
    "# concatenar os labels Y e X. Assim, quando excluirmos as linhas com dados faltantes, \n",
    "# o conjunto de labels será mantido. \n",
    "XY = np.concatenate((X,Y), axis = 1)\n",
    "\n",
    "# verificando a dimensão da concatenação, + 1 coluna (conjunto de labels)\n",
    "print('Dados concatenados: (Secom + Labels): {}.'.format(XY.shape))\n",
    "\n",
    "# excluindo as linhas que possuem algum dado faltante\n",
    "df_non_null = XY[~np.isnan(XY).any(axis=1)]\n",
    "print('Linhas com dados faltantes removidas. Novo número de linhas: %d.'\\\n",
    "      % df_non_null.shape[0])\n",
    "\n",
    "# separando o data-frame das Labels\n",
    "secom  = df_non_null[:, :558]\n",
    "print('Novo formato do Secom (desconcatenado das Labels): {}.'.format(secom.shape))\n",
    "\n",
    "labels = df_non_null[:, 558:559].reshape(978,)\n",
    "# labels = df_non_null[:, 558:559]\n",
    "print('Novo formato das Labels: {}.'.format(labels.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente padronize as colunas para media 0 e desvio padrao 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# scale() fornece média zero das colunas e desvio padrão = 1\n",
    "std_secom = scale(secom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PCA\n",
    "\n",
    "### Vamos encontrar 80% da variância e transformar os Dados. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# função PCA com sklearn\n",
    "def doPCA(x): # x é o argumento do número de componentes do PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=x)\n",
    "    pca.fit(std_secom)\n",
    "    return pca\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# executando o PCA com 558 componentes.\n",
    "pca = doPCA(std_secom.shape[1]) \n",
    "\n",
    "# os elementos em _var contém a total variância.\n",
    "_var = pca.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com 84 elementos temos a variância requisitada.\n",
      "The 84 principal components:\n"
     ]
    }
   ],
   "source": [
    "# armazena a variância.\n",
    "requested_var = 0\n",
    "\n",
    "# 80%  é a variância requisitada.\n",
    "lim = 0.80\n",
    "\n",
    "# iterações para encontrar a quantidade de dimensões que mantém\n",
    "# 80% da variância\n",
    "for i in range (len(_var)):\n",
    "    requested_var += _var[i]\n",
    "    if requested_var >= lim:\n",
    "        element = i\n",
    "        requested_var -= _var[i] # para ajuste do indice\n",
    "        break\n",
    "\n",
    "print('Com %d elementos temos a variância requisitada.' % (element))\n",
    "\n",
    "# repetindo o PCA com 80% da variância\n",
    "pca = doPCA(element) # element == 84 componentes\n",
    "\n",
    "# prints\n",
    "print('The %d principal components:' % (element))\n",
    "np.set_printoptions(precision=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para 80% da variância é necessário manter 84 elementos. \n",
    "\n",
    "#### Vamos transformar os dados para fazer o KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados PCA com 80%, formato: (978, 84).\n"
     ]
    }
   ],
   "source": [
    "secom_PCA_80 = pca.transform(std_secom)\n",
    "\n",
    "# formato dos dados com 80% da variância\n",
    "print('Dados PCA com 80%, formato: {}.'\\\n",
    "      .format(secom_PCA_80.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN com 5-fold externo e 3-fold interno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em fold #1: o melhor k é: 5\n",
      "Acurácia do Fold = 84.6938775510204%\n",
      "Em fold #2: o melhor k é: 5\n",
      "Acurácia do Fold = 95.40816326530613%\n",
      "Em fold #3: o melhor k é: 11\n",
      "Acurácia do Fold = 97.44897959183673%\n",
      "Em fold #4: o melhor k é: 5\n",
      "Acurácia do Fold = 95.38461538461539%\n",
      "Em fold #5: o melhor k é: 5\n",
      "Acurácia do Fold = 92.3076923076923%\n",
      "\n",
      "Acurácia do Método: 93.0486656200942%\n"
     ]
    }
   ],
   "source": [
    "# acurácia inicial\n",
    "acc = 0\n",
    "\n",
    "# separa os dados transformados  secom_PCA_80 em 5-fold\n",
    "kf_5 = KFold(n = len(labels), n_folds=5)\n",
    "\n",
    "#contador dos folds\n",
    "count = 0\n",
    "\n",
    "# para cada fold externo encontrar o melhor hiperparâmetro\n",
    "for train, test in kf_5:\n",
    "    count += 1\n",
    "    kf_3 = KFold(n = len(labels[train]), n_folds=3)\n",
    "    \n",
    "    # faz um grid search do melhor k em cada conjunto de treino do 5-fold\n",
    "    best_k, best_acc = 0,0\n",
    "        \n",
    "    for k in [1, 5, 11, 15, 21, 25]:\n",
    "        new_acc = 0\n",
    "        \n",
    "        for train_hp, test_hp in kf_3:\n",
    "            # define o knn            \n",
    "            knn = KNN(n_neighbors=k)\n",
    "            # treina secom_PCA_80\n",
    "            knn.fit(secom_PCA_80[train][train_hp], labels[train][train_hp]) \n",
    "            # avalia a acurária \n",
    "            new_acc += knn.score(secom_PCA_80[train][test_hp], labels[train][test_hp])\n",
    "        \n",
    "        # acurácia média do hiperparâmetro k\n",
    "        new_acc = new_acc/3\n",
    "        \n",
    "        # verifica se acurácia do hiperparametro k atual é a melhor\n",
    "        if (new_acc > best_acc):\n",
    "            best_acc = new_acc\n",
    "            best_k = k\n",
    "    \n",
    "    # imprime a melhor acurácia obtida pelo fold interno\n",
    "    print('Em fold #{}: o melhor k é: {}'.format(count, best_k))\n",
    "    \n",
    "    # ao fim do loop, calcula a acurácia para o melhor k\n",
    "    knn_out = KNN(n_neighbors=best_k)\n",
    "    \n",
    "    # treina com os valores do 5-fold externo\n",
    "    knn_out.fit(secom_PCA_80[train], labels[train]) \n",
    "    \n",
    "    # obtém a acurácia usando o conjunto de teste do 5-fold externo\n",
    "    print('Acurácia do Fold = {}%'.format(100*knn_out.score(secom_PCA_80[test],\\\n",
    "           labels[test])))\n",
    "   \n",
    "    # acha a acurácia média do método\n",
    "    acc += knn_out.score(secom_PCA_80[test], labels[test])\n",
    "\n",
    "acc /= 5    \n",
    "print('\\nAcurácia do Método: {}%'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "### Para o SVM RBF teste para C=2**(-5), 2**(0), 2**(5), 2**(10) e gamma= 2**(-15) 2**(-10) 2**(-5) 2**(0) 2**(5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em fold #1: o melhor c é: 0.031250 e gamma: 0.000031\n",
      "Acurácia do Fold = 84.6938775510204%\n",
      "Em fold #2: o melhor c é: 0.031250 e gamma: 0.000031\n",
      "Acurácia do Fold = 95.91836734693877%\n",
      "Em fold #3: o melhor c é: 0.031250 e gamma: 0.000031\n",
      "Acurácia do Fold = 97.44897959183673%\n",
      "Em fold #4: o melhor c é: 0.031250 e gamma: 0.000031\n",
      "Acurácia do Fold = 95.38461538461539%\n",
      "Em fold #5: o melhor c é: 0.031250 e gamma: 0.000031\n",
      "Acurácia do Fold = 92.3076923076923%\n",
      "\n",
      "Acurácia do Método: 93.15070643642073%\n"
     ]
    }
   ],
   "source": [
    "# acurácia inicial\n",
    "acc = 0\n",
    "\n",
    "# separa os dados transformados  std_secom em 5-fold\n",
    "kf_5 = KFold(n = len(labels), n_folds=5)\n",
    "\n",
    "#contador dos folds\n",
    "count = 0\n",
    "\n",
    "# para cada fold externo encontrar o melhor hiperparâmetro\n",
    "for train, test in kf_5:\n",
    "    count += 1\n",
    "    kf_3 = KFold(n = len(labels[train]), n_folds=3)\n",
    "    \n",
    "    # faz um grid search do melhor k em cada conjunto de treino do 5-fold\n",
    "    best_c, best_gamma, best_acc = 0,0,0\n",
    "        \n",
    "    for c in [2**-5, 2**0, 2**5, 2**10]:\n",
    "        for gamma in [2**-15, 2**-10, 2**-5, 2**0, 2**5]:\n",
    "            new_acc = 0\n",
    "            for train_hp, test_hp in kf_3:\n",
    "                # define o SVM classificador svc\n",
    "                svc = SVC(C=c, kernel='rbf', gamma=gamma)\n",
    "                # treina std_secom\n",
    "                svc.fit(std_secom[train][train_hp], labels[train][train_hp]) \n",
    "                # avalia a acurária \n",
    "                new_acc += svc.score(std_secom[train][test_hp], \\\n",
    "                                     labels[train][test_hp])\n",
    "        \n",
    "            # acurácia média do hiperparâmetro k\n",
    "            new_acc = new_acc/3\n",
    "        \n",
    "            # verifica se acurácia do hiperparametro k atual é a melhor\n",
    "            if (new_acc > best_acc):\n",
    "                best_acc = new_acc\n",
    "                best_c = c\n",
    "                best_gamma = gamma\n",
    "    \n",
    "    # imprime a melhor acurácia obtida pelo fold interno\n",
    "    print('Em fold #%d: o melhor c é: %.6f e gamma: %.6f'\n",
    "           % (count, best_c, best_gamma))\n",
    "    \n",
    "    # ao fim do loop, calcula a acurácia para o melhor c e gamma\n",
    "    svc_out = SVC(C=best_c, kernel='rbf', gamma=best_gamma)\n",
    "    \n",
    "    # treina com os valores do 5-fold externo\n",
    "    svc_out.fit(std_secom[train], labels[train]) \n",
    "    \n",
    "    # obtém a acurácia usando o conjunto de teste do 5-fold externo\n",
    "    print('Acurácia do Fold = {}%'.format(100*svc_out.score(std_secom[test],\\\n",
    "           labels[test])))\n",
    "   \n",
    "    # acha a acurácia média do método\n",
    "    acc += svc_out.score(std_secom[test], labels[test])\n",
    "\n",
    "acc /= 5    \n",
    "print('\\nAcurácia do Método: {}%'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Neural\n",
    "###  Para a rede neural, teste com 10, 20, 30 e 40 neuronios na camada escondida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em fold #1: o melhor número de neurônios na camada escondida é: 40\n",
      "Acurácia do Fold = 83.16326530612244%\n",
      "Em fold #2: o melhor número de neurônios na camada escondida é: 30\n",
      "Acurácia do Fold = 91.3265306122449%\n",
      "Em fold #3: o melhor número de neurônios na camada escondida é: 20\n",
      "Acurácia do Fold = 95.91836734693877%\n",
      "Em fold #4: o melhor número de neurônios na camada escondida é: 30\n",
      "Acurácia do Fold = 93.33333333333333%\n",
      "Em fold #5: o melhor número de neurônios na camada escondida é: 30\n",
      "Acurácia do Fold = 90.25641025641026%\n",
      "\n",
      "Acurácia do Método: 90.79958137100995%\n"
     ]
    }
   ],
   "source": [
    "# acurácia inicial\n",
    "acc = 0\n",
    "\n",
    "# separa os dados transformados  std_secom em 5-fold\n",
    "kf_5 = KFold(n = len(labels), n_folds=5)\n",
    "\n",
    "#contador dos folds\n",
    "count = 0\n",
    "\n",
    "# para cada fold externo encontrar o melhor hiperparâmetro\n",
    "for train, test in kf_5:\n",
    "    count += 1\n",
    "    kf_3 = KFold(n = len(labels[train]), n_folds=3)\n",
    "    \n",
    "    # faz um grid search do melhor k em cada conjunto de treino do 5-fold\n",
    "    best_neur, best_acc = 0,0\n",
    "        \n",
    "    for neur in [10, 20, 30, 40]:\n",
    "        new_acc = 0\n",
    "        \n",
    "        for train_hp, test_hp in kf_3:\n",
    "            # define o knn            \n",
    "            nn = MLP(solver='lbfgs', alpha=1, hidden_layer_sizes=neur, random_state=1)\n",
    "            # treina std_secom\n",
    "            nn.fit(std_secom[train][train_hp], labels[train][train_hp]) \n",
    "            # avalia a acurária \n",
    "            new_acc += nn.score(std_secom[train][test_hp], labels[train][test_hp])\n",
    "        \n",
    "        # acurácia média do hiperparâmetro k\n",
    "        new_acc = new_acc/3\n",
    "        \n",
    "        # verifica se acurácia do hiperparametro k atual é a melhor\n",
    "        if (new_acc > best_acc):\n",
    "            best_acc = new_acc\n",
    "            best_neur = neur\n",
    "    \n",
    "    # imprime a melhor acurácia obtida pelo fold interno\n",
    "    print('Em fold #{}: o melhor número de neurônios na camada escondida é: {}'.format(count, best_neur))\n",
    "    \n",
    "    # ao fim do loop, calcula a acurácia para o melhor k\n",
    "    nn_out = MLP(solver='lbfgs', alpha=1, hidden_layer_sizes=best_neur, random_state=1)\n",
    "    \n",
    "    # treina com os valores do 5-fold externo\n",
    "    nn_out.fit(std_secom[train], labels[train]) \n",
    "    \n",
    "    # obtém a acurácia usando o conjunto de teste do 5-fold externo\n",
    "    print('Acurácia do Fold = {}%'.format(100*nn_out.score(std_secom[test],\\\n",
    "           labels[test])))\n",
    "   \n",
    "    # acha a acurácia média do método\n",
    "    acc += nn_out.score(std_secom[test], labels[test])\n",
    "\n",
    "acc /= 5    \n",
    "print('\\nAcurácia do Método: {}%'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "\n",
    "### Para o RF, teste com n_featrues = 10, 15, 20, 25 e ntrees = 100, 200, 300 e 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em fold #1: o melhor n_feature é: 10.000000 e n_tree: 200.000000\n",
      "Acurácia do Fold = 84.6938775510204%\n",
      "Em fold #2: o melhor n_feature é: 10.000000 e n_tree: 100.000000\n",
      "Acurácia do Fold = 95.91836734693877%\n",
      "Em fold #3: o melhor n_feature é: 10.000000 e n_tree: 300.000000\n",
      "Acurácia do Fold = 97.44897959183673%\n",
      "Em fold #4: o melhor n_feature é: 10.000000 e n_tree: 300.000000\n",
      "Acurácia do Fold = 94.87179487179486%\n",
      "Em fold #5: o melhor n_feature é: 10.000000 e n_tree: 100.000000\n",
      "Acurácia do Fold = 92.3076923076923%\n",
      "\n",
      "Acurácia do Método: 93.04814233385663%\n"
     ]
    }
   ],
   "source": [
    "# acurácia inicial\n",
    "acc = 0\n",
    "\n",
    "# separa os dados transformados  std_secom em 5-fold\n",
    "kf_5 = KFold(n = len(labels), n_folds=5)\n",
    "\n",
    "#contador dos folds\n",
    "count = 0\n",
    "\n",
    "# para cada fold externo encontrar o melhor hiperparâmetro\n",
    "for train, test in kf_5:\n",
    "    count += 1\n",
    "    kf_3 = KFold(n = len(labels[train]), n_folds=3)\n",
    "    \n",
    "    # faz um grid search do melhor k em cada conjunto de treino do 5-fold\n",
    "    best_n_features, best_n_features, best_acc = 0,0,0\n",
    "        \n",
    "    for n_features in [10, 15, 20, 25]:\n",
    "        for n_trees in [100, 200, 300, 400]:\n",
    "            new_acc = 0\n",
    "            for train_hp, test_hp in kf_3:\n",
    "                # define o SVM classificador svc\n",
    "                rf = RF(max_features=n_features, max_depth=n_trees)\n",
    "                # treina std_secom\n",
    "                rf.fit(std_secom[train][train_hp], labels[train][train_hp]) \n",
    "                # avalia a acurária \n",
    "                new_acc += rf.score(std_secom[train][test_hp], \\\n",
    "                                     labels[train][test_hp])\n",
    "        \n",
    "            # acurácia média do hiperparâmetro k\n",
    "            new_acc = new_acc/3\n",
    "        \n",
    "            # verifica se acurácia do hiperparametro k atual é a melhor\n",
    "            if (new_acc > best_acc):\n",
    "                best_acc        = new_acc\n",
    "                best_n_features = n_features\n",
    "                best_n_trees    = n_trees\n",
    "    \n",
    "    # imprime a melhor acurácia obtida pelo fold interno\n",
    "    print('Em fold #%d: o melhor n_feature é: %.6f e n_tree: %.6f'\n",
    "           % (count, best_n_features, best_n_trees))\n",
    "    \n",
    "    # ao fim do loop, calcula a acurácia para o melhor c e gamma\n",
    "    rf_out = RF(max_features=best_n_features, max_depth=best_n_trees)\n",
    "    \n",
    "    # treina com os valores do 5-fold externo\n",
    "    rf_out.fit(std_secom[train], labels[train]) \n",
    "    \n",
    "    # obtém a acurácia usando o conjunto de teste do 5-fold externo\n",
    "    print('Acurácia do Fold = {}%'.format(100*rf_out.score(std_secom[test],\\\n",
    "           labels[test])))\n",
    "   \n",
    "    # acha a acurácia média do método\n",
    "    acc += rf_out.score(std_secom[test], labels[test])\n",
    "\n",
    "acc /= 5    \n",
    "print('\\nAcurácia do Método: {}%'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine\n",
    "### Para o GBM (ou XGB) teste para numero de arvores = 30, 70, e 100, com learning rate de 0.1 e 0.05, e profundidade da arvore=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em fold #1: o melhor learning rate é: 30.000000 e n_tree: 0.100000\n",
      "Acurácia do Fold = 84.18367346938776%\n",
      "Em fold #2: o melhor learning rate é: 100.000000 e n_tree: 0.100000\n",
      "Acurácia do Fold = 93.36734693877551%\n",
      "Em fold #3: o melhor learning rate é: 70.000000 e n_tree: 0.050000\n",
      "Acurácia do Fold = 97.44897959183673%\n",
      "Em fold #4: o melhor learning rate é: 30.000000 e n_tree: 0.050000\n",
      "Acurácia do Fold = 94.35897435897435%\n",
      "Em fold #5: o melhor learning rate é: 30.000000 e n_tree: 0.050000\n",
      "Acurácia do Fold = 92.3076923076923%\n",
      "\n",
      "Acurácia do Método: 92.33333333333334%\n"
     ]
    }
   ],
   "source": [
    "# acurácia inicial\n",
    "acc = 0\n",
    "\n",
    "# separa os dados transformados  std_secom em 5-fold\n",
    "kf_5 = KFold(n = len(labels), n_folds=5)\n",
    "\n",
    "#contador dos folds\n",
    "count = 0\n",
    "\n",
    "# para cada fold externo encontrar o melhor hiperparâmetro\n",
    "for train, test in kf_5:\n",
    "    count += 1\n",
    "    kf_3 = KFold(n = len(labels[train]), n_folds=3)\n",
    "    \n",
    "    # faz um grid search do melhor k em cada conjunto de treino do 5-fold\n",
    "    best_learning_rate, best_n_tree, best_acc = 0.1,0,0\n",
    "        \n",
    "    for ln in [0.1, 0.05]:\n",
    "        for n_trees in [30, 70, 100]:\n",
    "            new_acc = 0\n",
    "            for train_hp, test_hp in kf_3:\n",
    "                # define o GBM classificador \n",
    "                gbm = GBM(n_estimators=n_trees, learning_rate=ln, max_depth=5)\n",
    "                # treina std_secom\n",
    "                rf.fit(std_secom[train][train_hp], labels[train][train_hp]) \n",
    "                # avalia a acurária \n",
    "                new_acc += rf.score(std_secom[train][test_hp], \\\n",
    "                                     labels[train][test_hp])\n",
    "        \n",
    "            # acurácia média do hiperparâmetro k\n",
    "            new_acc = new_acc/3\n",
    "        \n",
    "            # verifica se acurácia do hiperparametro k atual é a melhor\n",
    "            if (new_acc > best_acc):\n",
    "                best_acc        = new_acc\n",
    "                best_learning_rate = ln\n",
    "                best_n_tree    = n_trees\n",
    "    \n",
    "    # imprime a melhor acurácia obtida pelo fold interno\n",
    "    print('Em fold #%d: o melhor learning rate é: %.6f e n_tree: %.6f'\n",
    "           % (count, best_n_tree, best_learning_rate))\n",
    "    \n",
    "    # ao fim do loop, calcula a acurácia para o melhor c e gamma\n",
    "    gbm_out = GBM(n_estimators=best_n_tree, learning_rate=best_learning_rate)\n",
    "    \n",
    "    # treina com os valores do 5-fold externo\n",
    "    gbm_out.fit(std_secom[train], labels[train]) \n",
    "    \n",
    "    # obtém a acurácia usando o conjunto de teste do 5-fold externo\n",
    "    print('Acurácia do Fold = {}%'.format(100*gbm_out.score(std_secom[test],\\\n",
    "           labels[test])))\n",
    "   \n",
    "    # acha a acurácia média do método\n",
    "    acc += gbm_out.score(std_secom[test], labels[test])\n",
    "\n",
    "acc /= 5    \n",
    "print('\\nAcurácia do Método: {}%'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [PYT3]",
   "language": "python",
   "name": "Python [PYT3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
